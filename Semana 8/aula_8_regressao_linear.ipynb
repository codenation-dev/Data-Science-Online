{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 8 - Regressão linear.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGIuOfqgw53f",
        "colab_type": "text"
      },
      "source": [
        "![Codenation](https://forum.codenation.com.br/uploads/default/original/2X/2/2d2d2a9469f0171e7df2c4ee97f70c555e431e76.png)\n",
        "\n",
        "__Autor__: Kazuki Yokoyama (kazuki.yokoyama@ufrgs.br)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi4xZxcfBA2U",
        "colab_type": "text"
      },
      "source": [
        "# Regressão linear\n",
        "\n",
        "![cover](https://miro.medium.com/max/1400/1*-hY-dDt5VlCDWeX7nmQfGw.png)\n",
        "\n",
        "Neste módulo, falaremos de um dos primeiros e mais simples modelos de predição, a regressão linear. Apesar da sua simplicidade, a regressão linear possui grande poder preditivo e ainda é a ferramenta padrão para muitas áreas como, por exemplo, Econometria. Além disso, esses modelos trazem alguns detalhes teóricos que são facilmente esquecidos ou ignorados, mas que devem ter a devida atenção."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAxxSlo3QrZV",
        "colab_type": "text"
      },
      "source": [
        "## Importação das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMxYy1NkQwW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import functools\n",
        "from math import sqrt\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as sct\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_squared_error, median_absolute_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNbPRHkKQyv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Algumas configurações para o matplotlib.\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.core.pylabtools import figsize\n",
        "\n",
        "\n",
        "figsize(12, 12)\n",
        "\n",
        "sns.set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8onCO86Q2Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvmMIPBrs5ph",
        "colab_type": "text"
      },
      "source": [
        "## _Trade-off viés-variância_\n",
        "\n",
        "Antes de falarmos sobre regressão, vale a pena introduzir o conceito do _trade-off_ viés-variância (_bias-variance trade-off_). O entendimento do _trade-off_ ajuda a compreender algumas decisões que podemos tomar ao escolher e comparar modelos de regressão.\n",
        "\n",
        "Para avaliar um modelo, podemos observar o quanto esse modelo erra ao fazer predições, de acordo com alguma funções de erro. É possível mostrar que o erro cometido por um modelo sempre pode ser decomposto em três partes: o viés, a variância e um erro irredutível:\n",
        "\n",
        "$$\\mathbb{E}[(f(x) - \\hat{f}(x))^{2}] = \\underbrace{\\text{Bias}(\\hat{f}(x))^{2}}_{\\text{viés}} + \\underbrace{\\text{Var}[\\hat{f}(x)]}_{\\text{variância}} + \\underbrace{\\varepsilon^{2}}_{\\text{erro irredutível}}$$\n",
        "\n",
        "onde\n",
        "\n",
        "$$\\text{Bias}(\\hat{f}(x)) = \\mathbb{E}[\\hat{f}(x)] - f(x)$$\n",
        "\n",
        "e\n",
        "\n",
        "$$\\text{Var}[\\hat{f}(x)] = \\mathbb{E}[\\hat{f}(x)^{2}] - \\mathbb{E}[\\hat{f}(x)]^{2}$$\n",
        "\n",
        "O viés se deve à escolha do modelo, que pode ter sido muito simples para o problema em mãos. Por exemplo, se temos um problema altamente não linear e escolhemos um modelos linear simples, introduziremos o nosso viés na escolha do modelo, resultando em predições fracas e uma baixa performance. Geralmente, o viés é relacionado _underfitting_ dos dados.\n",
        "\n",
        "A variância, por outro lado, está relacionada à complexidade do modelo. Isso se traduz em pequenas variações dos dados resultando em grandes variações nas predições, o que não deveria acontecer. Um modelo com alta variância, por exemplo um modelo de regressão com excessivas _features_ polinomiais, geralmente apresenta _overfitting_ dos dados.\n",
        "\n",
        "O erro irredutível é parte inerente do modelo e que não pode ser reduzido. Para reduzirmos o erro do modelo, devemos nos focar então em reduzir o viés ou a variância.\n",
        "\n",
        "Acontece que não podemos reduzir ambos simultaneamente: ao reduzirmos o viés, aumentamos a variância e vice-versa. Por isso trata-se de um _trade-off_. Devemos escolher reduzir o viés ou a variância (o que for possível com o modelo e o que levar aos melhores resultados)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QDPs5_qJwwL",
        "colab_type": "text"
      },
      "source": [
        "![mse-train-test](http://www.thefactmachine.com/wp-content/uploads/2015/04/14-Ridge-TestTrainingGraph.jpg)\n",
        "\n",
        "Note na figura acima como o MSE de treinamento é sempre descendente, enquanto o de teste tem a forma de um 'U'. Durante o treino, o modelo tenta se ajustar ao máximo ao conjunto de dados apresentado a ele (o conjunto de treinamento), levando a um \"superaprendizado\" desses dados. Chamamos isso de _overfitting_: o modelo não aprende a generalizar, e sim a decorar os dados apresentados durante o treinamento. Por outro lado, se o modelo for muito mais simples do que a realidade que se tenta modelar, observaremos _underfitting_: o modelo não consegue capturar as variações legítimas dos dados e, portanto, performa de forma não satisfatória.\n",
        "\n",
        "![overfitting-underfitting](https://miro.medium.com/max/1400/1*9hPX9pAO3jqLrzt0IE3JzA.png)\n",
        "\n",
        "O que estamos interessados é no erro cometido durante o teste (ou validação) do modelo, quando novos dados (não utilizados durante o treinamento) são apresentados ao modelo e sua performance é então avaliada. Quando o modelo tem pouca flexibilidade (alto viés e baixa variância), o modelo erra muito durante o teste, tendo alto erro e baixo desempenho. De acordo em que a flexibilidade do modelo aumenta (baixo viés e alta variância) o modelo começa a acertar mais (cometer menos erro) durante um tempo, e logo depois começa a errar mais novamente. O que queremos é exatamente o ponto intermediário onde ele erra o mínimo durante o teste. Esse é o ponto ótimo do modelo e os hiperparâmetros utilizados pelo modelo nesse momento são ótimos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD_wyTDK2YeS",
        "colab_type": "text"
      },
      "source": [
        "## Regressão linear\n",
        "\n",
        "Agora vamos analisar o modelo de regressão linear. Trata-se de um modelo antigo, com seus primeiros estudos feitos por Legendre e Gauss, mas que só teve a nomeação de _regressão_ dada por Sir Francis Galton em seus estudos sobre alturas de pais e filhos.\n",
        "\n",
        "Duas observações muito importantes antes de prosseguirmos:\n",
        "\n",
        "1. A regressão linear possui uma série de suposições estatísticas e propriedades que são importantes e tentaremos apresentar aqui.\n",
        "2. Apesar do nome, a regressão linear também lida com relações não lineares, basta introduzir variáveis polinomiais por exemplo.\n",
        "\n",
        "O problema que tentamos resolver aqui é um problema de regressão: dadas variáveis independentes, desejamos prever uma variável numérica dependente.\n",
        "\n",
        "Por exemplo, a partir de dados de sensores (temperatura, pressão, umidade etc), podemos estar interessados em predizer a resistência de um certo material. Os diversos nomes atribuídos a ambas variáveis dependentes e independentes no contexto da regressão são mostrados abaixo:\n",
        "\n",
        "![terminology](https://drive.google.com/uc?export=download&id=1-0KctX0PRWhvHSj_bBbw4EuzwDs0sTJ3)\n",
        "\n",
        "Em um problema de regressão, as variáveis independentes podem ser numéricas ou categóricas, enquanto a variável explicada é sempre numérica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73lbxqc8-x5m",
        "colab_type": "text"
      },
      "source": [
        "### Modelo teórico\n",
        "\n",
        "Vamos começar com o caso mais simples de regressão linear, onde temos apenas uma variável independente. Chamaremos essa variável de $X$ e a variável dependente de $Y$. Nesse cenário, podemos dizer que o valor esperado de $Y$ para um valor $X_{i}$ de $X$ tem a forma\n",
        "\n",
        "$$\\mathbb{E}[Y | X_{i}] = f(X_{i})$$\n",
        "\n",
        "Note que $f(X)$ é a \"verdade\", uma relação teórica com parâmetros desconhecidos.\n",
        "\n",
        "Podemos imaginar que essa relação é estreitamente linear e dizer que\n",
        "\n",
        "$$f(X_{i}) = \\beta_{1} + \\beta_{2} X_{i}$$\n",
        "\n",
        "Chamamos a função $f(X_{i})$ de função de regressão populacional, FRP, e ela geralmente é desconhecida, assim como todo parâmetro populacional. Em outras palavras, não conhecemos os parâmetros $\\beta_{0}$ e $\\beta_{1}$ que descrevem exatamente a relação entre $X$ e $Y$.\n",
        "\n",
        "![model](https://www.jmp.com/en_us/statistics-knowledge-portal/what-is-regression/simple-linear-regression-model/_jcr_content/par/styledcontainer_2069/par/lightbox_55ba/lightboxImage.img.gif/1562878184943.gif)\n",
        "\n",
        "Para um dado valor $X_{i}$ de $X$, observamos um determinado valor $Y_{i}$ de $Y$. Para não trabalharmos mais com $\\mathbb{E}[Y | X_{i}]$ e sim com $Y_{i}$, precisamos introduzir o termo de erro:\n",
        "\n",
        "$$Y_{i} = \\beta_{1} + \\beta_{2} X_{i} + u_{i}$$\n",
        "\n",
        "Como não conhecemos a FRP, vamos estimá-la a partir de uma função de regressão amostral, FRA. Podemos escrever a FRA como\n",
        "\n",
        "$$Y_{i} = \\hat{f}(X_{i}) = \\hat{\\beta}_{1} + \\hat{\\beta}_{2} X_{i} + \\hat{u}_{i}$$\n",
        "\n",
        "A relação entre a FRP (PRF), a FRA (SRF) e suas componentes é ilustrada abaixo:\n",
        "\n",
        "![relation](https://drive.google.com/uc?export=download&id=1I1gLeak5Vz8OVSd_hAoC1efo-XCDdc-x)\n",
        "\n",
        "Essa base teórica nos serve para entendermos de forma mais sólida o problema estatístico por baixo dos panos. Toda análise feita com apenas uma variável independente aqui se estende diretamente ao problema com múltiplas variáveis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq3r4UfDJtsh",
        "colab_type": "text"
      },
      "source": [
        "### Modelo prático\n",
        "\n",
        "Entendido o problema teórico, vamos partir para uma versão mais simples e prática do problema, já com múltiplas variáveis. O que queremos é estimar uma variável $y$ a partir de dados de $p$ variáveis $x$ através de um modelo de regressão. Isso pode ser escrito como\n",
        "\n",
        "$$\\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i1} + \\hat{\\beta}_{2} x_{i2} + \\cdots + \\hat{\\beta}_{p} x_{ip}$$\n",
        "\n",
        "onde $i$ denota a $i$-ésima observação do conjunto de treino.\n",
        "\n",
        "Como $\\hat{y}$ é uma aproximação para o verdadeiro $y$, então, para cada observação $i$ da amostra, existe um erro/resíduo associado $e_{i}$ entre o valor observado e a predição:\n",
        "\n",
        "$$e_{i} = y_{i} - \\hat{y}_{i}$$\n",
        "\n",
        "Na figura abaixo, a curva em azul denota $\\hat{y}$, enquanto os pontos vermelhos são os valores $y_{i}$ da amostra. As barras pretas entre $y_{i}$ e $\\hat{y}_{i}$ denotam os resíduos, ou erros de predição, $e_{i}$.\n",
        "\n",
        "![residual](https://uc-r.github.io/public/images/analytics/regression/sq.errors-1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5nJhuqON6uT",
        "colab_type": "text"
      },
      "source": [
        "A soma de todos $e_{i}$ ($i = 1, 2, \\cdots, n$) é chamado de soma quadrática dos resíduos, RSS (_Residual Sum of Squares_):\n",
        "\n",
        "$$\\text{RSS} = \\sum_{1 \\leq i \\leq n} e_{i}^{2} = \\sum_{1 \\leq i \\leq n} (y_{i} - \\hat{y}_{i})^{2}$$\n",
        "\n",
        "É uma variação dela que queremos minimizar durante o treino, o MSE (_Mean Squared Error_):\n",
        "\n",
        "$$\\text{MSE} = \\frac{1}{n} \\sum_{1 \\leq i \\leq n} e_{i}^{2} = \\frac{1}{n} \\sum_{1 \\leq i \\leq n} (y_{i} - \\hat{y}_{i})^{2}$$\n",
        "\n",
        "Em outras palavras, o problema que queremos resolver é minimizar a seguinte função:\n",
        "\n",
        "$$\\underset{\\hat{\\beta}_{0}, \\hat{\\beta}_{2}, \\cdots, \\hat{\\beta}_{p}}{\\text{minimizar}} \\text{MSE} = \\frac{1}{n} \\sum_{1 \\leq i \\leq n} (y_{i} - \\hat{\\beta}_{0} - \\sum_{1 \\leq j \\leq p} \\hat{\\beta}_{j}x_{ij})^{2}$$\n",
        "\n",
        "Apesar disso, para avaliar o nosso modelo após treinado, utilizamos o RMSE (_Root Mean Square Error_):\n",
        "\n",
        "$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{1 \\leq i \\leq n} e_{i}^{2} = \\sum_{1 \\leq i \\leq n} (y_{i} - \\hat{y}_{i})^{2}}$$\n",
        "\n",
        "O motivo para isso é que, devido suas propriedades analíticas (derivação etc), é bem mais fácil estimar os valores de $\\beta$ utilizando o MSE do que utilizando o RMSE.\n",
        "\n",
        "O RMSE é o erro-padrão dos erros. Ele é uma medida de quanto o nosso modelo erra ao predizer valores que não foram usadas durante o treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhlA16tdPnfV",
        "colab_type": "text"
      },
      "source": [
        "Um jeito de encontrar os valores de $\\hat{\\beta}_{0}, \\hat{\\beta}_{1}, \\cdots, \\hat{\\beta}_{p}$, ou seja, de treinar o nosso modelo é utilizando a equação normal:\n",
        "\n",
        "$$\\hat{\\beta} = (X^{T}X)^{-1} X^{T} y$$\n",
        "\n",
        "É muito comum utilizar a matriz pseudoinversa de Moore-Penrose, $X^{+}$, dada sua facilidade de computação e maior robustez em casos excepcionais:\n",
        "\n",
        "$$\\hat{\\beta} = X^{+} y$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9fVn_uucTJD",
        "colab_type": "text"
      },
      "source": [
        "Para ver isso na prática, vamos utilizar o _data set_ famoso de casas de Boston (sim, ele mesmo):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHKKG7jMcfJK",
        "colab_type": "code",
        "outputId": "a6530f4c-2c02-42b1-87f6-ea3daf1652f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "boston_dataset = load_boston()\n",
        "\n",
        "boston_features = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
        "boston_target = pd.DataFrame(boston_dataset.target, columns=[\"Price\"])\n",
        "\n",
        "boston = pd.concat([boston_features, boston_target], axis=1)\n",
        "\n",
        "boston.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX  ...    TAX  PTRATIO       B  LSTAT  Price\n",
              "0  0.00632  18.0   2.31   0.0  0.538  ...  296.0     15.3  396.90   4.98   24.0\n",
              "1  0.02731   0.0   7.07   0.0  0.469  ...  242.0     17.8  396.90   9.14   21.6\n",
              "2  0.02729   0.0   7.07   0.0  0.469  ...  242.0     17.8  392.83   4.03   34.7\n",
              "3  0.03237   0.0   2.18   0.0  0.458  ...  222.0     18.7  394.63   2.94   33.4\n",
              "4  0.06905   0.0   2.18   0.0  0.458  ...  222.0     18.7  396.90   5.33   36.2\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgfHVlXkdWW3",
        "colab_type": "code",
        "outputId": "db482b68-ff90-4a52-e616-142e3650e985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        }
      },
      "source": [
        "print(boston_dataset.DESCR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".. _boston_dataset:\n",
            "\n",
            "Boston house prices dataset\n",
            "---------------------------\n",
            "\n",
            "**Data Set Characteristics:**  \n",
            "\n",
            "    :Number of Instances: 506 \n",
            "\n",
            "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
            "\n",
            "    :Attribute Information (in order):\n",
            "        - CRIM     per capita crime rate by town\n",
            "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
            "        - INDUS    proportion of non-retail business acres per town\n",
            "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
            "        - NOX      nitric oxides concentration (parts per 10 million)\n",
            "        - RM       average number of rooms per dwelling\n",
            "        - AGE      proportion of owner-occupied units built prior to 1940\n",
            "        - DIS      weighted distances to five Boston employment centres\n",
            "        - RAD      index of accessibility to radial highways\n",
            "        - TAX      full-value property-tax rate per $10,000\n",
            "        - PTRATIO  pupil-teacher ratio by town\n",
            "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
            "        - LSTAT    % lower status of the population\n",
            "        - MEDV     Median value of owner-occupied homes in $1000's\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
            "\n",
            "This is a copy of UCI ML housing dataset.\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
            "\n",
            "\n",
            "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
            "\n",
            "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
            "prices and the demand for clean air', J. Environ. Economics & Management,\n",
            "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
            "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
            "pages 244-261 of the latter.\n",
            "\n",
            "The Boston house-price data has been used in many machine learning papers that address regression\n",
            "problems.   \n",
            "     \n",
            ".. topic:: References\n",
            "\n",
            "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
            "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpAl4akyh940",
        "colab_type": "text"
      },
      "source": [
        "Vamos começar separando o nosso conjunto de dados em dois grupos: treinamento (80%) e teste (20%)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9hdZPsfiFqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boston_features_train, boston_features_test, boston_target_train, boston_target_test = train_test_split(boston_features,\n",
        "                                                                                                        boston_target,\n",
        "                                                                                                        test_size=0.2,\n",
        "                                                                                                        random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJQLE2ECfFOU",
        "colab_type": "text"
      },
      "source": [
        "Vamos treinar o modelo com todos os dados do conjunto primeiro:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAWYUXCrfIbQ",
        "colab_type": "code",
        "outputId": "c8cc2210-c66e-48f2-d5e2-4750cbfac8cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "linear_regression = LinearRegression()\n",
        "\n",
        "linear_regression.fit(boston_features_train, boston_target_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18azFEr1LMH2",
        "colab_type": "text"
      },
      "source": [
        "Utilizaremos os dados do conjunto de teste para fazer predições. Note que nenhum desses dados de teste foram utilizados durante o treinamento do modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrT37Le_gRgs",
        "colab_type": "code",
        "outputId": "b2d686ff-ae67-472f-ec54-f6f2ef3406ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "predicted = linear_regression.predict(boston_features_test)\n",
        "\n",
        "predicted[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[28.99672362],\n",
              "       [36.02556534],\n",
              "       [14.81694405],\n",
              "       [25.03197915],\n",
              "       [18.76987992]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6LLMVcKLS7i",
        "colab_type": "text"
      },
      "source": [
        "Os verdadeiros valores de preço para os dados previstos acima são mostrados abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MNyZrrwjzs5",
        "colab_type": "code",
        "outputId": "9382a36d-fd67-42cd-a557-a9757588bd81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "boston_target_test.values[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[23.6],\n",
              "       [32.4],\n",
              "       [13.6],\n",
              "       [22.8],\n",
              "       [16.1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c86NH5qzLYMH",
        "colab_type": "text"
      },
      "source": [
        "Podemos verificar os valores do parâmetros estimados, excluindo o intercepto, através do atributo `coef_` disponível no modelo após o treinamento (i.e., após a chamada da função `fit()`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c1z0-KMhpy1",
        "colab_type": "code",
        "outputId": "f2a034bc-d942-4e03-ed50-ed9bb9e1b464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "linear_regression.coef_.round(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.10e-01,  3.00e-02,  4.00e-02,  2.78e+00, -1.72e+01,  4.44e+00,\n",
              "        -1.00e-02, -1.45e+00,  2.60e-01, -1.00e-02, -9.20e-01,  1.00e-02,\n",
              "        -5.10e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRuj7WmOLlZ-",
        "colab_type": "text"
      },
      "source": [
        "O valor do intercepto pode ser obtido através do atributo `intercept_`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GXPX3DikQYj",
        "colab_type": "code",
        "outputId": "e88d5fe1-c5ad-4a7b-dc0c-ac64ccaa4a11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "linear_regression.intercept_.round(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([30.25])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtMvNntPL5uS",
        "colab_type": "text"
      },
      "source": [
        "Para mostrar a matemática por trás do treinamento desses modelos, vamos criar um `np.ndarray` com as variáveis e os valores de preço:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQJmlfXuiKeg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boston_complete_train = np.c_[np.ones(boston_features_train.shape[0]), boston_features_train.values]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E37MtRvdMGkx",
        "colab_type": "text"
      },
      "source": [
        "E utilizar as operações matriciais e vetoriais disponíveis no NumPy para calcular a equação normal:\n",
        "\n",
        "$$\\hat{\\beta} = (X^{T}X)^{-1} X^{T} y$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxXideiIlFLm",
        "colab_type": "code",
        "outputId": "038956be-7384-4789-be23-0d2ec4c82629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "np.linalg.inv(boston_complete_train.T.dot(boston_complete_train)).dot(boston_complete_train.T).dot(boston_target_train.values).round(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.025e+01],\n",
              "       [-1.100e-01],\n",
              "       [ 3.000e-02],\n",
              "       [ 4.000e-02],\n",
              "       [ 2.780e+00],\n",
              "       [-1.720e+01],\n",
              "       [ 4.440e+00],\n",
              "       [-1.000e-02],\n",
              "       [-1.450e+00],\n",
              "       [ 2.600e-01],\n",
              "       [-1.000e-02],\n",
              "       [-9.200e-01],\n",
              "       [ 1.000e-02],\n",
              "       [-5.100e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS7LYh90MVir",
        "colab_type": "text"
      },
      "source": [
        "Repare como os valores dos parâmetros estimados acima são iguais aos valores retornados pelo modelo em `linear_regression`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl6Vnww9Me4n",
        "colab_type": "text"
      },
      "source": [
        "Uma outra forma de calcular os mesmos parâmetros é utilizar a função `lstsq()` também do NumPy. Ela calcula exatamente o produto da matriz pseudoinversa de Moore-Penrose pela variável dependente:\n",
        "\n",
        "$$\\hat{\\beta} = X^{+} y$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFhyTeL5lD-h",
        "colab_type": "code",
        "outputId": "60a266c9-340c-483a-f91d-c161f5c3f320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "coeff_svd, _res, _rank, _s = np.linalg.lstsq(boston_complete_train, boston_target_train.values, rcond=1e-6)\n",
        "\n",
        "coeff_svd.round(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.025e+01],\n",
              "       [-1.100e-01],\n",
              "       [ 3.000e-02],\n",
              "       [ 4.000e-02],\n",
              "       [ 2.780e+00],\n",
              "       [-1.720e+01],\n",
              "       [ 4.440e+00],\n",
              "       [-1.000e-02],\n",
              "       [-1.450e+00],\n",
              "       [ 2.600e-01],\n",
              "       [-1.000e-02],\n",
              "       [-9.200e-01],\n",
              "       [ 1.000e-02],\n",
              "       [-5.100e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkphxyJhM4tZ",
        "colab_type": "text"
      },
      "source": [
        "Novamente, chegamos aos mesmos valores estimados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pisml45M7KK",
        "colab_type": "text"
      },
      "source": [
        "Para finalizar, poderíamos ter utilizado a função `pinv()` que calcula a matriz pseudoinversa de Moore-Penrose (que depois podemos ainda multiplicar pela variável independente):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gEIA8S6ksbA",
        "colab_type": "code",
        "outputId": "0f436f7b-03fd-4f5d-ca61-a4b4a3178863",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "np.linalg.pinv(boston_complete_train).dot(boston_target_train.values).round(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.025e+01],\n",
              "       [-1.100e-01],\n",
              "       [ 3.000e-02],\n",
              "       [ 4.000e-02],\n",
              "       [ 2.780e+00],\n",
              "       [-1.720e+01],\n",
              "       [ 4.440e+00],\n",
              "       [-1.000e-02],\n",
              "       [-1.450e+00],\n",
              "       [ 2.600e-01],\n",
              "       [-1.000e-02],\n",
              "       [-9.200e-01],\n",
              "       [ 1.000e-02],\n",
              "       [-5.100e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUjW2KCHNUho",
        "colab_type": "text"
      },
      "source": [
        "A partir do vetor de parâmetroes estimados, podemos prever o valor do preço de uma casa a partir das variáveis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2vyCdjBlasY",
        "colab_type": "code",
        "outputId": "d9c5c4ea-4f6b-48b5-97fd-fe324ce68757",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "first_house = boston_complete_train[0, :]\n",
        "\n",
        "first_house_predicted = first_house.dot(coeff_svd)\n",
        "\n",
        "first_house_predicted"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([10.96952405])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8Rvyv0xrWfw",
        "colab_type": "text"
      },
      "source": [
        "## _Ridge_\n",
        "\n",
        "A regressão _ridge_ é uma forma de regularizar a regressão linear. O objetivo é aprender um modelo de regressão ao mesmo tempo em que mantém reduzidas as magnitudes dos parâmetros aprendidos.\n",
        "\n",
        "Para isso, adicionamos um termo de regularização à função de custo:\n",
        "\n",
        "$$J^{\\text{ridge}}(\\hat{\\beta}) = \\frac{1}{n} \\sum_{1 \\leq i \\leq n} (y_{i} - \\hat{\\beta}_{0} - \\sum_{1 \\leq j \\leq p} \\hat{\\beta}_{j}x_{ij})^{2} + \\underbrace{\\lambda \\sum_{1 \\leq j \\leq p} \\hat{\\beta}_{j}^{2}}_{\\text{termo de regularização}}$$\n",
        "\n",
        "Note como no termo de regularização, consideramos apenas os parâmetros $\\hat{\\beta}$ de índices 1 a $p$. Isso porque não queremos regularizar o intercepto ($\\hat{\\beta}_{0}$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FB2ZaVPRNAd",
        "colab_type": "text"
      },
      "source": [
        "O termo $\\lambda$ regula o quanto queremos regularizar o modelo. Para $\\lambda = 0$, a regressão _ridge_ se torna uma regressão linear sem regularização. Conforme aumentamos o valor de $\\lambda$, mais regularizado se torna o modelo, ou seja, menores serão os valores de $\\beta$ estimados.\n",
        "\n",
        "O parâmetro $\\lambda$ é um hiperparâmetro do modelo e deve ser ajustado com métodos como _cross-validation_. A escolha do $\\lambda$ correto é vital para o bom desempenho do _ridge_.\n",
        "\n",
        "Apesar de tornar os parâmetros estimados valores muito pequenos, a regressão _ridge_ não os zera por completo. Em outras palavras, não somos capazes de fazer seleção de variáveis naturalmente com a regressão _ridge_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCF2unm7RmJ4",
        "colab_type": "text"
      },
      "source": [
        "O poder da regressão _ridge_ vem justamente do _trade-off bias-variance_. A regressão sem regularização ($\\lambda = 0$) possui grande variância e pouco viés. À medida em que aumentamos $\\lambda$, a variância diminui a uma taxa maior do que o viés aumenta, reduzindo assim o MSE total. A partir de certo ponto, o viés começa a aumentar mais rápido do que a variância aumenta, tornando a aumenta o MSE. Assim, existe um valor intermediário onde o MSE da regressão _ridge_ encontra seu mínimo e que é menor que o MSE da regressão linear sem regularização.\n",
        "\n",
        "Importante notar que realizamos a regularização somente durante o treino. Em todas etapas seguintes (teste, validação, e predição), devemos avaliar a performance do modelo __sem__ regularização.\n",
        "\n",
        "Outro ponto importante é que devemos escalar os dados antes de treinar a regressão _ridge_ utilizando o `StandardScaler`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIMvmHlgRpPN",
        "colab_type": "text"
      },
      "source": [
        "A regressão _ridge_ também é chamada de regressão $\\ell_{2}$, pois utiliza a norma-$\\ell_{2}$:\n",
        "\n",
        "$$\\|\\hat{\\beta}\\|_{2} = \\sqrt{\\sum_{j} \\hat{\\beta}_{j}^{2}}$$\n",
        "\n",
        "Vamos utilizar a regressão _ridge_ abaixo:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82yGLqHIiq4k",
        "colab_type": "text"
      },
      "source": [
        "Começamos escalando os dados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tLJpU-bited",
        "colab_type": "code",
        "outputId": "dac198d4-85b2-4620-86da-04540ecda950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "boston_features_train_scaled = scaler.fit_transform(boston_features_train)\n",
        "\n",
        "boston_features_train_scaled[:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.28770177, -0.50032012,  1.03323679, -0.27808871,  0.48925206,\n",
              "        -1.42806858,  1.02801516, -0.80217296,  1.70689143,  1.57843444,\n",
              "         0.84534281, -0.07433689,  1.75350503],\n",
              "       [-0.33638447, -0.50032012, -0.41315956, -0.27808871, -0.15723342,\n",
              "        -0.68008655, -0.43119908,  0.32434893, -0.62435988, -0.58464788,\n",
              "         1.20474139,  0.4301838 , -0.5614742 ],\n",
              "       [-0.40325332,  1.01327135, -0.71521823, -0.27808871, -1.00872286,\n",
              "        -0.40206304, -1.6185989 ,  1.3306972 , -0.97404758, -0.60272378,\n",
              "        -0.63717631,  0.06529747, -0.65159505]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFNPUGt6O1Hy",
        "colab_type": "code",
        "outputId": "6dec2b04-e61d-4273-bf9a-4bb2bf92fbd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "ridge_regression = Ridge(alpha=1, solver=\"cholesky\")\n",
        "\n",
        "ridge_regression.fit(boston_features_train_scaled, boston_target_train)\n",
        "\n",
        "ridge_regression.intercept_, ridge_regression.coef_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([22.79653465]),\n",
              " array([[-0.99218679,  0.6777488 ,  0.2522143 ,  0.72248078, -1.99083465,\n",
              "          3.15157218, -0.17726162, -3.04502895,  2.17324941, -1.69555879,\n",
              "         -2.02783351,  1.127197  , -3.59897667]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbH9hNntR86x",
        "colab_type": "text"
      },
      "source": [
        "Note como os valores dos parâmetros estimados são bem próximos de zero (exceto o intercepto), mas nenhum exatamente zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp0kEpx0FxH4",
        "colab_type": "text"
      },
      "source": [
        "![mse-lambda](https://drive.google.com/uc?export=download&id=196MmaQswPNPhBt46zttNJbffFny3bBGH)\n",
        "\n",
        "A figura acima mostra a relação entre o erro (MSE) e o valor de $\\lambda$. A curva preta representa a variância, a curva preta representa o viés, a linha tracejada representa o erro irredutível, e a curva rosa representa o MSE, ou seja, a soma da variância com o quadrado do viés e o erro irredutível.\n",
        "\n",
        "Note como, até certo ponto, a variância diminui mais rápido do que o viés aumenta. Depois o viés aumenta muito mais rapidamente do que a variância consegue diminuir. Em algum ponto intermediário, encontramos o valor ótimo de $\\lambda$, quando o erro total, MSE, é mínimo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3FaBuKbWzXA",
        "colab_type": "text"
      },
      "source": [
        "## LASSO\n",
        "\n",
        "LASSO (_Least Absolute Shrinkage and Selection Operator_) é outra forma de regularização de regressão. Em vez de utilizar a norma-$\\ell_{2}$, o LASSO utiliza a norma-$\\ell_{1}$:\n",
        "\n",
        "$$\\|\\hat{\\beta}\\|_{1} = \\sqrt{\\sum_{j} \\| \\hat{\\beta}_{j} \\|}$$\n",
        "\n",
        "A função de custo com o novo termo de regularização adicionado se torna:\n",
        "\n",
        "$$J^{\\text{LASSO}}(\\hat{\\beta}) = \\frac{1}{n} \\sum_{1 \\leq i \\leq n} (y_{i} - \\hat{\\beta}_{0} - \\sum_{1 \\leq j \\leq p} \\hat{\\beta}_{j}x_{ij})^{2} + \\underbrace{\\lambda \\sum_{1 \\leq j \\leq p} \\| \\hat{\\beta}_{j} \\|}_{\\text{termo de regularização}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60ltRPqJXxs-",
        "colab_type": "text"
      },
      "source": [
        "Essa pequena diferença entre as funções de custo do _ridge_ e do LASSO é o suficiente para termos um comportamente bastante diferente. Ao contrário do _ridge_, o LASSO é capaz de zerar completamente parâmetros de variáveis com pouca relevância para o modelo, gerando um modelo esparso (algumas das variáveis têm parâmetros nulos). Essa é uma forma de seleção de variáveis embutida no próprio modelo.\n",
        "\n",
        "As mesmas observações do _ridge_ valem para o LASSO:\n",
        "\n",
        "* Devemos escalar as variáveis antes de treinar o modelo do LASSO.\n",
        "* Apenas utilizamos a função de custo com o termo de regularização adicionado durante o treino. Em etapas consecutivas (teste, validação, predição etc) devemos avaliar o desempenho do modelo com a função de custo simples, __sem__ regularização."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiTusu_BYv49",
        "colab_type": "code",
        "outputId": "d9e59bdc-acde-45bb-c71a-0c251568f368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "lasso_regression = Lasso(alpha=0.5)\n",
        "\n",
        "lasso_regression.fit(boston_features_train_scaled, boston_target_train)\n",
        "\n",
        "lasso_regression.intercept_, lasso_regression.coef_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([22.79653465]),\n",
              " array([-0.30737992,  0.        , -0.        ,  0.40396576, -0.        ,\n",
              "         3.32512039, -0.        , -0.3333949 , -0.        , -0.        ,\n",
              "        -1.46416614,  0.80194371, -3.53546088]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92Z36MzLmOqW",
        "colab_type": "code",
        "outputId": "36ed2282-96a8-4ced-cd68-b6830efa85de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "boston_dataset.feature_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
              "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2P-c4ooZCBP",
        "colab_type": "text"
      },
      "source": [
        "Note como alguns parâmetros foram estimados como exatamente zero, removendo totalmente a respectiva variável do modelo final. As variáveis \"sobreviventes\" (não nulas) são as consideradas relevantes para a predição."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "calgfJ_lZeIF",
        "colab_type": "text"
      },
      "source": [
        "## Validação de regressão\n",
        "\n",
        "Existe um conjunto de métricas para validarmos o desempenho dos modelos de regressão. Essas métricas podem ser utilizadas para comparar modelos e também para ajustar o valor de hiperparâmetros como o fator de regularização $\\lambda$.\n",
        "\n",
        "Vamos falar de alguns deles:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXLvma6ObB7v",
        "colab_type": "text"
      },
      "source": [
        "### $R^{2}$\n",
        "\n",
        "O $R^{2}$, ou coeficiente de determinação, é uma métrica clássica para validação de regressão. Ele é definido como\n",
        "\n",
        "$$R^{2} = 1 - \\frac{\\sum_{1 \\leq i  \\leq n} (y_{i} - \\hat{y}_{i})^{2}}{\\sum_{1 \\leq i  \\leq n} (y_{i} - \\bar{y})^{2}}$$\n",
        "\n",
        "Isso nada mais é que a razão da variância explicada pelo modelo pela variância total dos dados.\n",
        "\n",
        "O $R^{2}$ é um valor que vai de 0 a 1. Quanto mais próximo de zero, menos o modelo explica a variância total, ou seja, menor é o desempenho do modelo. Quanto mais próximo de um, maior é a parcela da variância explicada pelo modelo, ou seja, melhor o modelo.\n",
        "\n",
        "Importante notar que o $R^{2}$ não é, na verdade, o quadrado de alguma medida $R$, e portanto pode ser negativo. Isso pode acontecer, por exemplo, se o seu modelo não inclui o intercepto (e o modelo performa pior que o esperado). Se você estiver presenciando um $R^{2}$ negativo, verifique se o intercepto está sendo incluído no modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUMVpWkHdgQF",
        "colab_type": "text"
      },
      "source": [
        "O $R^{2}$ tem dois problemas: quanto mais variáveis há no modelo, maior o $R^{2}$ naturalmente. Um jeito de inflar artificialmente o desempenho do modelo é adicionando variáveis ao modelo, mesmo que elas não sejam de fato muito relevantes.\n",
        "\n",
        "O segundo problema é decorrente do primeiro: não conseguimos comparar modelos com números diferentes de variáveis de forma justa. Não temos como saber se um possível $R^{2}$ maior no modelo com mais variáveis é devido a um melhor desempenho autêntico ou somente ao seu número maior de variáveis.\n",
        "\n",
        "Para resolver isso, existe o $R^{2}$ ajustado:\n",
        "\n",
        "$$R^{2}_{\\text{ajustado}} = 1 - \\frac{n-1}{n - (k + 1)} (1 - R^{2})$$\n",
        "\n",
        "onde $n$ é o número de observações (tamanho da amostra) e $k + 1$ é o número de parâmetros sendo estimados, incluindo o intercepto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gD9YIPaXhUOo",
        "colab_type": "code",
        "outputId": "47d6bce1-af34-489c-d6a8-ecd5e90642bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "r2_score(boston_target_test, predicted)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6687594935356307"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVE6ii2C897B",
        "colab_type": "text"
      },
      "source": [
        "### _Mean Squared Error_\n",
        "\n",
        "Uma outra forma de medir o desempenho de um modelo é através do já visto MSE:\n",
        "\n",
        "$$\\text{MSE} = \\frac{1}{n} \\sum_{1 \\leq i \\leq n} (y_{i} - \\hat{y}_{i})^{2}$$\n",
        "\n",
        "Esse valor, ao contrário do $R^{2}$, é sempre estritamente não negativo (sendo zero no caso de um ajuste perfeito aos dados). Quanto mais próximo de zero, melhor o desempenho do modelo, pois as diferenças entre as predições e os reais valores é menor.\n",
        "\n",
        "Esse valor sozinho não passa muitas informações, e portanto o MSE é melhor aproveitado ao comparar modelos. Por exemplo, podemos escolher o valor de $\\lambda$ em uma regressão _ridge_ ou LASSO que leve ao menor MSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwSc18Dj_w12",
        "colab_type": "code",
        "outputId": "0da61c8c-8c02-4701-d2d3-5894d5a47266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mean_squared_error(boston_target_test, predicted)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24.291119474973616"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NROCHvsTBJRf",
        "colab_type": "text"
      },
      "source": [
        "### _Median Absolute Error_\n",
        "\n",
        "Outra medida interessante é o MedAE, a mediana dos erros de predição. Essa métrica é robusta à presença de _outliers_, pois considera os quantis dos resultados ao invés da média, que é muito mais sensível à variações extremas.\n",
        "\n",
        "O MedAE é computado como:\n",
        "\n",
        "$$\\text{MedAE} = \\text{mediana}\\{y_{1} - \\hat{y}_{1}, y_{2} - \\hat{y}_{2}, \\cdots, y_{n} - \\hat{y}_{n}\\}$$\n",
        "\n",
        "Esse valor assume qualquer valor real e, como toda medida de erro, quanto menor, melhor o modelo. Assim como o MSE, esse valor por si só não diz muita coisa, e é melhor utilizado ao comparar modelos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG8bANcCCHcL",
        "colab_type": "code",
        "outputId": "404e938e-dad8-4b32-caa2-be6edbaabe89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "median_absolute_error(boston_target_test, predicted)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3243319064124535"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14MxN64cw704",
        "colab_type": "text"
      },
      "source": [
        "## Referências\n",
        "\n",
        "* [Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/)\n",
        "\n",
        "* [Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
        "\n",
        "* [Introduction to Machine Learning Algorithms: Linear Regression](https://towardsdatascience.com/introduction-to-machine-learning-algorithms-linear-regression-14c4e325882a)\n",
        "\n",
        "* [7 Classical Assumptions of Ordinary Least Squares (OLS) Linear Regression](https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/)\n",
        "\n",
        "* [The Gauss-Markov Theorem and BLUE OLS Coefficient Estimates](https://statisticsbyjim.com/regression/gauss-markov-theorem-ols-blue/)\n",
        "\n",
        "* [Tikhonov regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization)\n",
        "\n",
        "* [Ridge Regression for Better Usage](https://towardsdatascience.com/ridge-regression-for-better-usage-2f19b3a202db)\n",
        "\n",
        "* [Lasso](https://en.wikipedia.org/wiki/Lasso_(statistics))\n",
        "\n",
        "* [Understanding Regression Error Metrics in Python](https://www.dataquest.io/blog/understanding-regression-error-metrics/)\n",
        "\n",
        "* [Understand Regression Performance Metrics](https://becominghuman.ai/understand-regression-performance-metrics-bdb0e7fcc1b3)"
      ]
    }
  ]
}